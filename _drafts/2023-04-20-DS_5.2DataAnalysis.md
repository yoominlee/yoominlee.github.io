---
layout: post
title: ITE4005_DS > (9) 5.2 Data analysis and pre-processing part 2
subtitle: Chapter5, Mining Frequent Patterns, Association and Correlations
categories: DataScience
tags: [DataScience, ITE4005]
use_math: true
---

# [Data Science]2: Getting to Know Your Data part2

목차

~~(저번시간)~~

- **~~Data Objects and Feature Types~~**
- **~~Basic Statistical Descriptions of Data~~**
    
    (이번시간)
    
- **Data Visualization**
- **Measuring Data Similarity and Dissimilarity**
- **Summary**

## **Graphic Displays of Basic Statistical Descriptions**

![1][1]
### Histogram Analysis

- Bar 형태로 frequencies 나타냄.
- 각 카테고리에 떨어지는 분포 보여
- x축에는 수치, 카테고리, 범위 올 수 있음
    - 카테고리나 범위의 경우 x축에서 구간 겹치는 부분 있으면 안됨
    - 비는 구간이 있어도 안됨
        
        = categories must be adjacent
        

![2][2]

### **Quantile Plot**

- sorted data를 오름차순으로 display
    - 전체적인 behavior와 unusual 한 경우(occurrences)를 모두 볼 수 있다.
- $f_i$에 100을 곱한 수가 퍼센트 의미.
    
    (예를들어 f-value가 0.25면 25%의 위치임)
    
    - ($f_1, x_1$), ($f_2, x_2$) … ($f_100, x_100$) 처럼 표기할 수 있음
    - f는 분위수 (Quantile)를 의미하고,
    - x는 sorted 된 값

![3][3]

### **Quantile-Quantile (Q-Q) Plot**

- 한 feature와 다른 데이터셋의 feature를 비교할 수 있다.

![4][4]

위 그래프의 예로,

데이터 값은 핸드폰의 가격,

각 축은 각각 테크노마트의 가격, 이마트의 가격이 될 수 있음

즉, 하나의 common feature에 대해 비교 가능

추가적으로 QQplot에는 추가적인 Quantile 정보가 없기 때문에 그래프 내부에 Q1 등 표기 해줌

또한 다시 그래프를 보면 중앙 부분에서 y=x 그래프 보다 y축 쪽에 더 가까워진 부분을 볼 수 있음. 

이것은 같은것에 대해 y가 더 비싸다는걸 나타낸다.

하나의 shared feature에 대해 두개의 데이터셋을 비교할 수 있다.

### **Scatter Plot**

- 두 특징쌍에 대해 2D space에 plot
- 두 특징 간의 관계에 대해 보여줌

![5][5]

그래프에서 볼 수 있는 것은 개당 가격이 85달러 정도가 될 때 까지는 가격이 올라갈수록 많이 팔렸는데, 그 이후로는 다시 판매량이 감소하는 것을 볼 수 있다.

데이터들의 **cluster, outlier**들 확인할 수 있다. 

![6][6]

위에서 볼 수 있듯 

양의 상관 관계를 보이기도 하고, 음의 상관 관계를 보이기도 한다.

하단에 보이듯, 절반에서는 양의 상관 관계, 나머지 절반에서는 음의 상관 관계를 나타내기도 함.

![7][7]

이런 경우는 uncorrelated data 혹은 correlation이 없다고 한다.

---

### **Similarity and Dissimilarity**

- Similarity

두 데이터 object가 얼마나 유사한지에 대한 numerical(수치적인) 측정(measure)

비슷할수록 수치 올라감

보통 [0,1] 범위로 떨어짐. (%로 표현할수도 있긴 함)

- Dissimilarity (distance)

두 데이터 object가 얼마나 다른지에 대한 numerical(수치적인) 측정(measure)

비슷할수록 값이 내려감

최소 dissimilarity는 보통 [0, $\infin$ ]

Proximity refers to a similarity or dissimilarity

integrated 컨셉임

### Matrix for Data

- Data matrix (n-by-p)

![8][8]

p 차원(특징) 가지고 있는 n개의 data point 

Two modes

- Dissimilarity (distance) matrix  (n-by-n)

n개의 데이터를 표현하는 다른 방법

모든 data 쌍에 대한 distance가 나타나 있음

![9][9]

두 삼각형 부분 중 한쪽만 필요하다.

보통 distance는 대칭이다(symmetric). 항상은 아님

distance function이 symmetric하지 않은 경우 행렬의 모든 부분 채워야 함

similarity matrix를 생각해볼 수 있음

그러면 대각선 부분의 값이 0이 아닌 1로 채워지게 될것임

각 타입별 proximity를 측정하는법에 대해 배움

- nominal features

보통 이름을 나타냄 (categorical, discrete)

P개의 nominal feature가 있다고 가정

**방법 1: simple matching**

(i, j에 대해) m을 계산하는 것이다.

m: number of matched features

p가 5개의 feature가 있고, 이 중 3개가 같은 값을 가지고 있다고 할 때,

![10][10]

d는 distance, sim은 similarity

p-m은 number of mismatch를 의미.

**방법 2: 하나의 nominal feature를 표현하기 위해 여러 개의 binary feature를 사용하는** 

![11][11]

각 특징마다 binary feature를 만드는 것

### **Proximity Measure for Binary Features**

위와 아이디어 비슷함

![12][12]

총 p개의 binary feature 있음

각 알파벳의 의미는 matching(p,t), mismatching(r,s)인 feature의 수

Proximity Measure의 두 타입 볼 수 있다

- Symmetric binary feature을 위한

male female 처럼 동일하게 중요한 경우 (방법1과 유사)

![13][13]

- Asymmetric binary feature을 위한

암이 있는지 아닌지, 코로나인지 아닌지 같은 경우, 암이 있는 1 case가 더 중요 → 이런경우 asymmetruic 한 경우 라고 함

![14][14]

위와 다 같지만 t만 제거 된 형태

예제

Asymmetric binary feature의 Dissimilarity

gender는 symmetric feature고, 나머지가 asymmetric이기 때문에 지금은 gender 빼고 고려.

그리고 분모가 q + r + s 를 계산하는것이긴 하지만, 이는 곧 전체 p에서 t를 뺀거랑 동일하니까 후자로 계산해도 됨!

주의해야 할 부분이, dissimilarity가 아니라 similarity를 계산하는 경우에도, 이건 asymmetric한 경우기 때문에 Y로 동일한 경우는 count 하고 N으로 동일한 경우는 제거한다.

### **Proximity Measure for Numeric Features**

numerical 특징들은 먼저 normalization 이나 standardize를 해야함!

(나이와 salary의 스케일은 많이 다르므로 salary에 highly focus 할 것임. 이를 방지하기 위해 normalize)

(normalization도 Z-score 등 다양한 방법 있는데 다음 챕터에서 소개할 것이어서 우선 SKIP)

![15][15]

distance도 종류 많음.

**Distance on Numeric Data**

<방법1>

- Minkowski distance

![16][16]

i, j는 data point

p개의 feature.

h는 하이퍼파라미터. 프로그래머가 설정

- Minkowski distance의 스페셜 케이스

![17][17]

만약 h=2라면 Euclidean Distance(L-2 norm)와 동일

h = $ \infin $ 인 경우 (맨 우측 수식 보면),

p개의 데이터 중 각 feature 차이가 가장 큰 것 선택

>결국 Minkowski distance는 numeric 값을 가진 두 종류의 feature를 가진 여러 데이터들의 거리를 비교하는 것??

>h=1인 경우 ㄴ자로 두 특징의 거리 합함

h=2인 경우 삼각형 처럼 사선 거리들 합

h가 무한이면 가장 큰 값을 선택한다는건, 1에서 본 ㄴ자를 합하는게 아니라 둘 중 긴쪽의 거리를 선택한다는것.

<아이패드 필기 있는 #20>

- Minkowski distance 특징
    - d(i,j) > 0 (i,j 같아서 0인 경우 제외)
    - symmetry(대칭) $\therefore$ d(i,j) = d(j,i)
    - d(i,j) ≤ d(i,k) + d(k,j) 삼각부등식(triangle inequality)
    
    ⇒ 위 세 properties 만족하는 distance라면 “Metric” 하다고 함
    
    → Minkowski 셋다 만족
    

<방법2>

- Cosine similarity

Minkowski는 distance, dissimilarity에 더 집중했었음.

이건 similarity에 집중!

![18][18]

(위 예시는 Document들이 n-dimensional vector로 표현되어있음)

위 예시 보면, document 들의 값에는 차이가 좀 있어 보이지만, term frequency는 다르지만, contents는 유사하다고 판단

<< 수식 with 필기 >>

>(1,3), (4,9) 둘의 distance는 이전방법으로 연산 시 멀다고 판단 됨.

하지만 cosine similarity로 계산 시 두 data point 유사하다고 판단.

⇒ Distance 대신 두 데이터의 angle에 집중함으로써 orientation, direction에 집중.

(예시 skip)

### Distance for Ordinal Features

: order 있는 feature들 (ex, grade, size, …)

아이디어는, 값들을 rank로 바꿈

(예를 들어, A+, A0, B+, B0 … 일때, B+을 3으로 변경)

>$r_{if}$는 전체 데이터 중 i번째 data인 사람의 여러 feature중 f번째 feature의 값을 의미.

(i = data 순번, f = feature 순번)

이후 $z_{if} = \frac{M_f-1}{r_{if}-1}$ 를 사용해 rank의 값들을 [0,1]로 바꿈

→ 이 변환된 값들로 Minkowski, Cosine distance 등 사용 가능

### Features of Mixed Type

이전까지 다양한 type의 feature들의 similarity 및 distance 구하는 법 배움. 

하지만 실제 data는 여러 feature type이 섞여있음.

>다양한 type

Nominal, symmetric binary, asymmetric binary, numeric, ordinal

Weighted average를 사용할 수 있다!

![19][19]

$\delta$ : 각 feature의 importance, scale 등에 의한 weight

→ 이를 통해 final distance 얻을 수 있다.

Summary

- Data feature types: nominal, binary, ordinal, numerical (interval-scaled, ratio-scaled)
- Gain insight into the data by:
    - Basic statistical data description: central tendency, dispersion
    - 5 numbers summary, visualized by a boxplot.
- Visualizations
    - Scatter plot, QQ plot, histogram, etc...
- Measure data similarity
    - Minkowski Distance and its special cases
    - Cosine similarity
    - ....



![20][20]


---


[1]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/1.png
[2]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/2.png
[3]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/3.png
[4]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/4.png
[5]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/5.png
[6]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/6.png
[7]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/7.png
[8]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/8.png
[9]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/9.png
[10]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/10.png
[11]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/11.png
[12]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/12.png
[13]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/13.png
[14]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/14.png
[15]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/15.png
[16]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/16.png
[17]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/17.png
[18]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/18.png
[19]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/19.png
[20]: /assets/images/post_img/2023-04-20-DS_5.2DataAnalysis/20.png


출처 : 2023-1 ITE4005 수업  






